import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pickle
import os
from datetime import datetime
import matplotlib.pyplot as plt
import optuna
import warnings
warnings.filterwarnings('ignore')

# CUDA ì„¤ì • ë° í˜¸í™˜ì„± ì²´í¬
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA ë²„ì „: {torch.version.cuda}")
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    
    # RTX 5090 í˜¸í™˜ì„± ì²´í¬
    try:
        test_tensor = torch.randn(10, 10).cuda()
        test_result = test_tensor.sum()
        device = torch.device('cuda')
        print(f"âœ… CUDA ì •ìƒ ì‘ë™")
    except Exception as e:
        print(f"âŒ CUDA ì—ëŸ¬ ê°ì§€: {e}")
        print("CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
        device = torch.device('cpu')
else:
    device = torch.device('cpu')

print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

class ANNModel(nn.Module):
    def __init__(self, input_size, layer_config, dropout_rate=0.2, l1_reg=0.01, l2_reg=0.01):
        super(ANNModel, self).__init__()
        
        layers = []
        prev_size = input_size
        
        # íˆë“  ë ˆì´ì–´ êµ¬ì„±
        for i, layer_size in enumerate(layer_config):
            layers.append(nn.Linear(prev_size, layer_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))
            prev_size = layer_size
        
        # ì¶œë ¥ ë ˆì´ì–´
        layers.append(nn.Linear(prev_size, 1))
        
        self.model = nn.Sequential(*layers)
        self.l1_reg = l1_reg
        self.l2_reg = l2_reg
    
    def forward(self, x):
        return self.model(x)
    
    def get_l1_l2_loss(self):
        l1_loss = 0
        l2_loss = 0
        for param in self.parameters():
            l1_loss += torch.sum(torch.abs(param))
            l2_loss += torch.sum(param ** 2)
        return self.l1_reg * l1_loss + self.l2_reg * l2_loss

class AdvancedHyperparameterOptimizer:
    def __init__(self, data_path='HPNT_IQR_VIF5.xlsx'):
        self.data_path = data_path
        self.X_train = None
        self.X_val = None
        self.y_train = None
        self.y_val = None
        self.scaler = StandardScaler()
        self.best_model = None
        self.best_params = None
        self.best_score = float('inf')
        self.all_results = []
        
    def load_and_preprocess_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘...")
        
        df = pd.read_excel(self.data_path)
        print(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {df.shape}")
        
        # ê²°ì¸¡ì¹˜ ì œê±°
        df = df.dropna(subset=['ì‘ì—…ì†Œìš”ì‹œê°„_1'])
        
        # íŠ¹ì„± ì„ íƒ
        numeric_features = [
            'ì…í•­ì‹œê°„', 'ì…í•­ì›”', 'ì…í•­ë¶„ê¸°', 'ì…í•­ë…„ë„', 'ì…í•­íšŸìˆ˜',
            'ì´í†¤ìˆ˜', 'ì„ ì„', 'ì–‘í•˜', 'ì í•˜', 'ì–‘ì í•˜ë¬¼ëŸ‰', 'shift'
        ]
        
        categorical_features = ['ì„ ì‚¬', 'ì…í•­ìš”ì¼', 'ì…í•­ê³„ì ˆ', 'ROUTE', 'ì˜ˆì„ ', 'ë„ì„ ']
        target = 'ì‘ì—…ì†Œìš”ì‹œê°„_1'
        
        # ìˆ˜ì¹˜í˜• íŠ¹ì„± ì²˜ë¦¬
        X_numeric = df[numeric_features].fillna(0)
        
        # ë²”ì£¼í˜• íŠ¹ì„± ì²˜ë¦¬ (ì›í•« ì¸ì½”ë”©)
        X_categorical = pd.DataFrame()
        for feature in categorical_features:
            if feature in df.columns:
                df[feature] = df[feature].fillna('Unknown')
                dummies = pd.get_dummies(df[feature], prefix=feature)
                X_categorical = pd.concat([X_categorical, dummies], axis=1)
        
        # íŠ¹ì„± ê²°í•©
        X = pd.concat([X_numeric, X_categorical], axis=1)
        y = df[target].values
        
        print(f"ì „ì²˜ë¦¬ í›„ íŠ¹ì„± ìˆ˜: {X.shape[1]}")
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # ì •ê·œí™”
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        
        # í…ì„œë¡œ ë³€í™˜
        self.X_train = torch.FloatTensor(X_train_scaled).to(device)
        self.X_val = torch.FloatTensor(X_val_scaled).to(device)
        self.y_train = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)
        self.y_val = torch.FloatTensor(y_val.reshape(-1, 1)).to(device)
        
        self.input_size = X_train_scaled.shape[1]
        print(f"í›ˆë ¨ ë°ì´í„°: {self.X_train.shape}, ê²€ì¦ ë°ì´í„°: {self.X_val.shape}")
        
    def train_model(self, config, verbose=False):
        """ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ"""
        model = ANNModel(
            input_size=self.input_size,
            layer_config=config['layer_config'],
            dropout_rate=config['dropout_rate'],
            l1_reg=config['l1_reg'],
            l2_reg=config['l2_reg']
        ).to(device)
        
        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])
        criterion = nn.MSELoss()
        
        train_dataset = TensorDataset(self.X_train, self.y_train)
        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
        
        best_val_loss = float('inf')
        patience = 20
        patience_counter = 0
        
        for epoch in range(config['epochs']):
            model.train()
            train_loss = 0
            
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y) + model.get_l1_l2_loss()
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            # ê²€ì¦
            model.eval()
            with torch.no_grad():
                val_outputs = model(self.X_val)
                val_loss = criterion(val_outputs, self.y_val).item()
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    break
            
            if verbose and (epoch + 1) % 50 == 0:
                print(f'ì—í¬í¬ [{epoch+1}/{config["epochs"]}], ê²€ì¦ ì†ì‹¤: {val_loss:.4f}')
        
        # ìµœê³  ëª¨ë¸ ìƒíƒœ ë³µì›
        model.load_state_dict(best_model_state)
        
        # ìµœì¢… í‰ê°€
        model.eval()
        with torch.no_grad():
            val_pred = model(self.X_val).cpu().numpy()
            val_y_np = self.y_val.cpu().numpy()
            
            val_mse = mean_squared_error(val_y_np, val_pred)
            val_mae = mean_absolute_error(val_y_np, val_pred)
            val_r2 = r2_score(val_y_np, val_pred)
        
        return model, {'val_mse': val_mse, 'val_mae': val_mae, 'val_r2': val_r2}
    
    def optuna_optimization(self, n_trials=100):
        """Optunaë¥¼ ì‚¬ìš©í•œ ë² ì´ì§€ì•ˆ ìµœì í™”"""
        print(f"\nğŸ”¬ Optuna ë² ì´ì§€ì•ˆ ìµœì í™” ì‹œì‘ ({n_trials}íšŒ ì‹œë„)...")
        
        def objective(trial):
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
            n_layers = trial.suggest_int('n_layers', 1, 4)
            
            layer_config = []
            for i in range(n_layers):
                if i == 0:
                    layer_size = trial.suggest_categorical(f'layer_{i+1}', [16, 32, 64, 128, 256])
                else:
                    prev_size = layer_config[i-1]
                    max_size = min(prev_size, 256)
                    layer_size = trial.suggest_categorical(f'layer_{i+1}', [16, 32, 64, 128, 256])
                layer_config.append(layer_size)
            
            config = {
                'layer_config': layer_config,
                'learning_rate': trial.suggest_categorical('learning_rate', [0.001, 0.01, 0.1]),
                'dropout_rate': trial.suggest_categorical('dropout_rate', [0.1, 0.2, 0.3, 0.4, 0.5]),
                'l1_reg': trial.suggest_categorical('l1_reg', [0.0001, 0.001, 0.01]),
                'l2_reg': trial.suggest_categorical('l2_reg', [0.0001, 0.001, 0.01]),
                'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),
                'epochs': trial.suggest_categorical('epochs', [200, 300, 400, 500])
            }
            
            try:
                model, metrics = self.train_model(config)
                
                # ê²°ê³¼ ì €ì¥
                result = {'config': config.copy(), 'metrics': metrics, 'method': 'optuna'}
                self.all_results.append(result)
                
                # ìµœê³  ëª¨ë¸ ì—…ë°ì´íŠ¸
                if metrics['val_mse'] < self.best_score:
                    self.best_score = metrics['val_mse']
                    self.best_params = config.copy()
                    self.best_model = model
                    print(f"ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸! MSE: {self.best_score:.4f}, RÂ²: {metrics['val_r2']:.4f}")
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                del model
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
                
                return metrics['val_mse']
                
            except Exception as e:
                print(f"ì‹œë„ ì‹¤íŒ¨: {e}")
                return float('inf')
        
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=n_trials)
        
        print(f"Optuna ìµœì í™” ì™„ë£Œ! ìµœê³  MSE: {study.best_value:.4f}")
        
    def grid_search_fast(self):
        """ë¹ ë¥¸ ê·¸ë¦¬ë“œ ì„œì¹˜ (ì£¼ìš” ì¡°í•©ë§Œ)"""
        print("\nğŸ” ë¹ ë¥¸ ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹œì‘...")
        
        # í•µì‹¬ ì¡°í•©ë§Œ ì„ íƒ
        configs = [
            # 1 layer
            {'layer_config': [256], 'learning_rate': 0.01, 'dropout_rate': 0.2, 'l1_reg': 0.01, 'l2_reg': 0.01, 'batch_size': 64, 'epochs': 300},
            {'layer_config': [128], 'learning_rate': 0.01, 'dropout_rate': 0.2, 'l1_reg': 0.01, 'l2_reg': 0.01, 'batch_size': 32, 'epochs': 400},
            
            # 2 layers  
            {'layer_config': [256, 128], 'learning_rate': 0.01, 'dropout_rate': 0.2, 'l1_reg': 0.01, 'l2_reg': 0.01, 'batch_size': 64, 'epochs': 300},
            {'layer_config': [128, 64], 'learning_rate': 0.01, 'dropout_rate': 0.3, 'l1_reg': 0.01, 'l2_reg': 0.01, 'batch_size': 32, 'epochs': 400},
            
            # 3 layers
            {'layer_config': [128, 64, 32], 'learning_rate': 0.01, 'dropout_rate': 0.2, 'l1_reg': 0.01, 'l2_reg': 0.01, 'batch_size': 64, 'epochs': 400},
            {'layer_config': [64, 32, 16], 'learning_rate': 0.01, 'dropout_rate': 0.3, 'l1_reg': 0.01, 'l2_reg': 0.01, 'batch_size': 32, 'epochs': 500},
            
            # 4 layers
            {'layer_config': [32, 32, 16, 16], 'learning_rate': 0.01, 'dropout_rate': 0.2, 'l1_reg': 0.01, 'l2_reg': 0.01, 'batch_size': 32, 'epochs': 500},
        ]
        
        for i, config in enumerate(configs):
            print(f"ê·¸ë¦¬ë“œ ì„œì¹˜ ì§„í–‰: {i+1}/{len(configs)} - {config['layer_config']}")
            
            try:
                model, metrics = self.train_model(config, verbose=True)
                
                result = {'config': config.copy(), 'metrics': metrics, 'method': 'grid_search'}
                self.all_results.append(result)
                
                if metrics['val_mse'] < self.best_score:
                    self.best_score = metrics['val_mse']
                    self.best_params = config.copy()
                    self.best_model = model
                    print(f"ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸! MSE: {self.best_score:.4f}")
                
                del model
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"ì—ëŸ¬: {e}")
                continue
    
    def random_search(self, n_trials=30):
        """ëœë¤ ì„œì¹˜"""
        print(f"\nğŸ² ëœë¤ ì„œì¹˜ ì‹œì‘ ({n_trials}íšŒ)...")
        
        layer_options = [
            [256], 
            [128, 256], 
            [64, 128, 256], 
            [32, 64, 128], 
            [16, 32, 64, 128]
        ]
        
        for i in range(n_trials):
            # numpy.random.choice ëŒ€ì‹  random.choice ì‚¬ìš©
            import random
            config = {
                'layer_config': random.choice(layer_options),
                'learning_rate': random.choice([0.001, 0.01, 0.1]),
                'dropout_rate': random.choice([0.1, 0.2, 0.3, 0.4, 0.5]),
                'l1_reg': random.choice([0.0001, 0.001, 0.01]),
                'l2_reg': random.choice([0.0001, 0.001, 0.01]),
                'batch_size': random.choice([32, 64, 128]),
                'epochs': random.choice([200, 300, 400, 500])
            }
            
            print(f"ëœë¤ ì„œì¹˜: {i+1}/{n_trials}")
            
            try:
                model, metrics = self.train_model(config)
                
                result = {'config': config.copy(), 'metrics': metrics, 'method': 'random_search'}
                self.all_results.append(result)
                
                if metrics['val_mse'] < self.best_score:
                    self.best_score = metrics['val_mse']
                    self.best_params = config.copy()
                    self.best_model = model
                    print(f"ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸! MSE: {self.best_score:.4f}")
                
                del model
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"ì—ëŸ¬: {e}")
                continue
    
    def run_all_optimizations(self):
        """ëª¨ë“  ìµœì í™” ë°©ë²• ì‹¤í–‰"""
        print("ğŸš€ ëª¨ë“  ìµœì í™” ë°©ë²• ì‹¤í–‰ ì‹œì‘!")
        
        # 1. ë¹ ë¥¸ ê·¸ë¦¬ë“œ ì„œì¹˜
        self.grid_search_fast()
        
        # 2. ëœë¤ ì„œì¹˜  
        self.random_search(30)
        
        # 3. Optuna ë² ì´ì§€ì•ˆ ìµœì í™”
        try:
            self.optuna_optimization(50)
        except ImportError:
            print("Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. pip install optunaë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        except Exception as e:
            print(f"Optuna ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        
        self.print_final_results()
    
    def print_final_results(self):
        """ìµœì¢… ê²°ê³¼ ì¶œë ¥"""
        if not self.all_results:
            print("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*100)
        print("ğŸ† ìµœì¢… ìµœì í™” ê²°ê³¼")
        print("="*100)
        
        # ê²°ê³¼ ì •ë ¬
        sorted_results = sorted(self.all_results, key=lambda x: x['metrics']['val_mse'])
        
        print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤ (ìƒìœ„ 10ê°œ):")
        print("-" * 100)
        
        for i, result in enumerate(sorted_results[:10]):
            config = result['config']
            metrics = result['metrics']
            method = result.get('method', 'unknown')
            
            print(f"{i+1:2d}. [{method:12s}] ë ˆì´ì–´: {str(config['layer_config']):20s} "
                  f"MSE: {metrics['val_mse']:8.4f} "
                  f"MAE: {metrics['val_mae']:8.4f} "
                  f"RÂ²: {metrics['val_r2']:7.4f}")
        
        print(f"\nğŸ¯ ìµœì¢… ìš°ìŠ¹ ëª¨ë¸:")
        print(f"   ë°©ë²•: {sorted_results[0].get('method', 'unknown')}")
        print(f"   êµ¬ì¡°: {self.best_params['layer_config']}")
        print(f"   í•™ìŠµë¥ : {self.best_params['learning_rate']}")
        print(f"   ë“œë¡­ì•„ì›ƒ: {self.best_params['dropout_rate']}")
        print(f"   ë°°ì¹˜í¬ê¸°: {self.best_params['batch_size']}")
        print(f"   ì—í¬í¬: {self.best_params['epochs']}")
        print(f"   ì„±ëŠ¥ - MSE: {self.best_score:.4f}")
        
        # ë°©ë²•ë³„ ì„±ëŠ¥ ë¹„êµ
        methods_performance = {}
        for result in self.all_results:
            method = result.get('method', 'unknown')
            mse = result['metrics']['val_mse']
            if method not in methods_performance:
                methods_performance[method] = []
            methods_performance[method].append(mse)
        
        print(f"\nğŸ“Š ë°©ë²•ë³„ í‰ê·  ì„±ëŠ¥:")
        for method, mses in methods_performance.items():
            avg_mse = np.mean(mses)
            min_mse = np.min(mses)
            print(f"   {method:15s}: í‰ê·  MSE {avg_mse:.4f}, ìµœê³  MSE {min_mse:.4f}")
    
    def save_best_model(self, save_dir='models'):
        """ìµœê³  ëª¨ë¸ ì €ì¥"""
        if self.best_model is None:
            print("ì €ì¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        os.makedirs(save_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ëª¨ë¸ ì €ì¥
        model_path = os.path.join(save_dir, f'best_ann_model_{timestamp}.pth')
        torch.save({
            'model_state_dict': self.best_model.state_dict(),
            'config': self.best_params,
            'scaler': self.scaler,
            'input_size': self.input_size,
            'best_score': self.best_score
        }, model_path)
        
        # ê²°ê³¼ ì €ì¥
        results_path = os.path.join(save_dir, f'all_results_{timestamp}.pkl')
        with open(results_path, 'wb') as f:
            pickle.dump({
                'all_results': self.all_results,
                'best_params': self.best_params,
                'best_score': self.best_score
            }, f)
        
        print(f"\nâœ… ì €ì¥ ì™„ë£Œ:")
        print(f"   ëª¨ë¸: {model_path}")
        print(f"   ê²°ê³¼: {results_path}")
        
        return model_path, results_path

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    optimizer = AdvancedHyperparameterOptimizer('HPNT_IQR_VIF5.xlsx')
    
    # ë°ì´í„° ë¡œë“œ
    optimizer.load_and_preprocess_data()
    
    # ëª¨ë“  ìµœì í™” ë°©ë²• ì‹¤í–‰
    optimizer.run_all_optimizations()
    
    # ìë™ìœ¼ë¡œ ìµœê³  ëª¨ë¸ ì €ì¥
    optimizer.save_best_model()
    
    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")

if __name__ == "__main__":
    main()